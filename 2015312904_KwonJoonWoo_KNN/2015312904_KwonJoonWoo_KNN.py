# -*- coding: utf-8 -*-
"""2015312904_KwonJoonWoo_KNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OBp0hk6jBwb6p55ZH9WyAszNfi62Ci4r
"""

import nltk
import json
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score,precision_score
from sklearn.metrics import recall_score, f1_score

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Setting 1 - File I/O:
path = './bbc_articles_test.json'
with open(path,'r') as f:
  json_testdata = json.load(f)

path = './bbc_articles_train.json'
with open(path,'r') as f:
  json_traindata = json.load(f)

keylist_test = json_testdata.keys()
keylist_train = json_traindata.keys()

newpath = './2015312904_KwonJoonWoo_KNN.txt'
f = open(newpath,'w')

#----------------------Main code--------------------
# Setting 2
sample_list = ['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','NNPS']
temp_article = []
train_article = []
test_article = []
train_labels = []
test_labels = []

# TF-IDF : Train data
# Combining all 240 articles in one list and making labels list.

for i in keylist_train:
  for temp in json_traindata[i]:
    temp_article.append(''.join(temp))
    train_labels.append(i)
print('Original: {}'.format(temp_article))

# Check if tags are in sample list
for temp in temp_article:
  tokens = word_tokenize(temp)
  tagged_tokens = nltk.pos_tag(tokens)
  train_article.append(' '.join([i[0] for i in tagged_tokens if i[1] in sample_list]))
print('After Tags Verification: {}'.format(train_article))

# Create TF-IDF of train set
tfidfvect_train = TfidfVectorizer()
tfidfvect_train.fit_transform(train_article)
train_TFIDF = tfidfvect_train.transform(train_article).toarray().tolist()

print("Num of Train data: ",len(train_TFIDF))
print("Vector dim : ", len(train_TFIDF[0]))

del temp_article [:]

#-----------------------------------------------------
# TF-IDF : Test data
# Combining all 60 articles in one list
for i in keylist_test:
  for temp in json_testdata[i]:
    temp_article.append(''.join(temp))
    test_labels.append(i)
print('Original: {}'.format(temp_article))

# Check if tags are in sample list
for temp in temp_article:
  tokens = word_tokenize(temp)
  tagged_tokens = nltk.pos_tag(tokens)
  test_article.append(' '.join([i[0] for i in tagged_tokens if i[1] in sample_list]))
print('After Tags Verification: {}'.format(test_article))

# Create TF-IDF of train set
test_TFIDF = tfidfvect_train.transform(test_article).toarray().tolist()

print("Num of Test data: ",len(test_TFIDF))
print("Vector dim : ", len(test_TFIDF[0]))

#-----------------------------------------------------
# KNN classifier

KNN_classifier = KNeighborsClassifier(n_neighbors = 2, weights = 'distance',
                                      metric = 'euclidean')
KNN_classifier.fit(train_TFIDF,train_labels)
predict_test = KNN_classifier.predict(test_TFIDF)

#-----------------------------------------------------
# Round down to the fourth digit after the decimal point.
num = 4 
def digits(digits,num):
  new_digits = digits
  for i in range(0,num):
    new_digits = new_digits * 10

  temp = new_digits - int(new_digits)
  final = new_digits - temp

  for i in range(0,num):
    final = final /10
  final = round(final,num)

  return final
#-----------------------Results-----------------------
# Confusion matrix
mat_confusion = confusion_matrix(test_labels,predict_test)
print(mat_confusion)

f.write('Confusion matrix :\n')
for temp in mat_confusion:
  for i in temp:
    f.write('{}\t'.format(i))
  f.write("\n")

# Accuracy
acr = accuracy_score(test_labels,predict_test)
new_acr = digits(acr*100,num)
f.write("\nAccuracy : {}%\n".format(new_acr))

# Macro averaging precision
mac_precision = precision_score(test_labels,predict_test, average = 'macro')
mac_precision = digits(mac_precision*100,num)
f.write('\nMacro averaging precision : {}%\n'.format(mac_precision))

# Micro averaging precision
mic_precision = precision_score(test_labels,predict_test, average = 'micro')
mic_precision = digits(mic_precision*100,num)
f.write('Micro averaging precision : {}%\n'.format(mic_precision))

# Macro averaging recall
recall_macro = recall_score(test_labels,predict_test, average = 'macro')
recall_macro = digits(recall_macro*100, num)
f.write('\nMacro averaging recall : {}%\n'.format(recall_macro))

# Micro averaging recall
recall_micro = recall_score(test_labels,predict_test, average = 'micro')
recall_micro = digits(recall_micro*100, num)
f.write('Micro averaging recall : {}%\n'.format(recall_micro))

# Macro averaging f1-score
f1_macro = f1_score(test_labels,predict_test, average = 'macro')
f1_macro = digits(f1_macro*100, num)
f.write('\nMacro averaging f1-score : {}%\n'.format(f1_macro))

# Micro averaging f1-score
f1_micro = f1_score(test_labels,predict_test, average = 'micro')
f1_micro = digits(f1_micro*100, num)
f.write('Micro averaging f1-score : {}%\n'.format(f1_micro))
f.close()

from google.colab import drive
drive.mount('/content/drive')